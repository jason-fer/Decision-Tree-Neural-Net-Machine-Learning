
LECTURE ALGORITHMS:
MakeSubtree(set of training instances D) 
 C = DetermineCandidateSplits(D) 
 if stopping criteria met 
   make a leaf node N
   determine class label/probabilities for N
 else 
   make an internal node N
   S = FindBestSplit(D, C) 
   for each outcome k of S
     Dk = subset of instances that have outcome k
     kth child of N = MakeSubtree(Dk) 
 return subtree rooted at N


-splits on nominal features have one branch per value 
-splits on numeric features use a threshold 

Numeric candidate splits:
-given a set of training instances D and a specific feature Xi
-sort the values of Xi in D
-evaluate split thresholds in intervals between instances of 
different classes 
-could use midpoint of each considered interval as the threshold
-C4.5 instead picks the largest value of Xi in the entire training set that 
does not exceed the midpoint


// Run this subroutine for each numeric feature at each node of DT induction 
DetermineCandidateNumericSplits(set of training instances D, feature Xi) 
 C = {} // initialize set of candidate splits for feature Xi
 S = partition instances in D into sets s1 ... sV where the instances in each
   set have the same value for Xi
 let vj denote the value of Xi for set sj
 sort the sets in S using vj as the key for each sj

 for each pair of adjacent sets sj, sj+1 in sorted S
   if sj and sj+1 contain a pair of instances with different class labels
     // assume were using midpoints for splits 
     add candidate split Xi <= (vj + vj+1)/2 to C
 return C


OrdinaryFindBestSplit(set of training instances D, set of candidate splits C) 
 maxgain = negative infinity
 for each split S in C
   gain = InfoGain(D, S) 
   if gain > maxgain
     maxgain = gain
     Sbest = S

 return Sbest

LookaheadFindBestSplit(set of training instances D, set of candidate splits C) 
 maxgain = negative infinity
 for each split S in C
   gain = EvaluateSplit(D, C, S) 
   if gain > maxgain
     maxgain = gain
     Sbest = S
 return Sbest

EvaluateSplit(D, C, S)
 if a split on S separates instances by class (i.e. ) HD (Y | S) = 0
   // no need to split further 
   return H_D(Y) - H_D(Y | S)
 else 
   for outcomes k in set {1, 2} of S // let's assume binary splits
     // see what the splits at the next level would be
     Dk = subset of instances that have outcome k
     Sk = OrdinaryFindBestSplit(Dk, C - S) 
     // return information gain that would result from this 2-level subtree
   return HD(Y) - HD(Y | S,S1,S2)


LECTURE EQUATIONS:
1-entropy equation (one for nominal, one for boolean)
2-conditional entropy? (do i need it?)
3-information gain
4-if info gain is better than average, use gain ratio